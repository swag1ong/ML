{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Decision Tree\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in\n",
    "each one. They are conceptually simple yet powerful.\n",
    "\n",
    "## Regression Tree (CART)\n",
    "\n",
    "To simplify matters, we will restrict our attention to recursive binary partitions. We first split the space into two regions,\n",
    "and model the response by the mean of Y in each region. We choose the variable and split-point to achieve the best fit, then\n",
    "one or both of these split regions are split into two more regions, and this process is continued until some stopping rules are met.\n",
    "\n",
    "A key advantage of the recursive binary tree is its interpretability. The feature space partition is fully described by a single tree\n",
    "\n",
    "### Goal\n",
    "\n",
    "Assume we have observed N sample pairs $\\{x_i, y_i\\}$, where $x_i = [x_{i1}, ..., x_{id}]^T$ has d dimensions. Our goal is to\n",
    "create an algorithm that automatically decide on the splitting variables and splitting points.\n",
    "\n",
    "## Solution\n",
    "\n",
    "Suppose we end up having M regions (leaves) $R_1, R_2,..., R_M$ and we model the prediction in each region as a constant $c_m$\n",
    "\n",
    "$\\hat f (x_i) = \\sum_{m=1}^{M} c_m I(x_i \\in R_m)$\n",
    "\n",
    "This equation sum over all region and only the region that contains $x_i$ will output $c_m$\n",
    "\n",
    "If we have loss function $L = \\sum_{i=1}^{N} (y_i - \\hat f (x_i))^2 = \\sum_{i=1}^{N} (y_i - \\sum_{m=1}^{M} c_m I(x_i \\in R_m))^2$\n",
    "\n",
    "In order to minimize this loss function with respect to $c_j$, we take the derivative of L w.r.t $c_j$\n",
    "\n",
    "$\\implies \\frac{\\partial L}{\\partial c_j} = 2\\sum_{i=1}^{N} (y_i - \\sum_{m=1}^{M} c_m I(x_i \\in R_m)) (I(x_i \\in R_j)) = 0$\n",
    "\n",
    "$\\implies \\underbrace{\\sum_{i=1}^{N} y_i I(x_i \\in R_j)}_{\\text{sum of all } y_i \\text{ in } R_j} = \\sum_{i=1}^{N}\\sum_{m=1}^{M} c_m I(x_i \\in R_m) I(x_i \\in R_j)$\n",
    "\n",
    "$\\implies \\underbrace{\\sum_{i=1}^{N} y_i I(x_i \\in R_j)}_{\\text{sum of all } y_i \\text{ in } R_j} = \\sum_{i=1}^{N}\\sum_{m=1}^{M} c_m I(x_i \\in R_m) I(x_i \\in R_j)$\n",
    "\n",
    "If we look at right hand side equation, $I(x_i \\in R_j)$ can determine the value of $\\sum_{m=1}^{M} c_m I(x_i \\in R_m)$\n",
    "$\\implies \\sum_{i=1}^{N}\\sum_{m=1}^{M} c_m I(x_i \\in R_m) I(x_i \\in R_j) = \\sum_{i=1}^{N} c_j I(x_i \\in R_j) I(x_i \\in R_j) =\n",
    "\\sum_{i=1}^{N} c_j I(x_i \\in R_j)$ Since:\n",
    "\n",
    "  \\begin{equation}\n",
    "    \\sum_{m=1}^{M} c_m I(x_i \\in R_m) =\n",
    "    \\begin{cases}\n",
    "      c_j I(x_i \\in R_j), &  I(x_i \\in R_j) = 1\\\\\n",
    "      0, & I(x_i \\in R_j) = 0\n",
    "    \\end{cases}\n",
    "  \\end{equation}\n",
    "\n",
    "$\\implies \\hat c_j = \\frac{\\overbrace{\\sum_{i=1}^{N} y_i I(x_i \\in R_j)}^{\\text{sum of all } y_i \\text{ in } R_j}}\n",
    "{\\underbrace{\\sum_{i=1}^{N} c_j I(x_i \\in R_j)}_{\\text{all the samples in } R_j}}$ = the mean of $y \\in R_j$ = $\\bar R_j$\n",
    "\n",
    "However, finding the best binary partition in terms of minimize sum of squares is generally infeasible, hence, we proceed with a greedy algorithm\n",
    "\n",
    "Starting with all data, consider a splitting variable j and splitting point s. Then:\n",
    "\n",
    "$R_1 (j, s) = {x_i; x_{ij} \\leq s}$, $R_2 (j, s) = {x_i; x_{ij} > s}$\n",
    "\n",
    "Then we seek the splitting variable j and splitting point s that solve:\n",
    "\n",
    "$\\text{min}_{j, s} [\\text{min}_{c_1} \\sum_{x_i \\in R_1 (j, s)} (y_i - \\hat f (x_i))^2 + \\text{min}_{c_2} \\sum_{x_i \\in R_2 (j, s} (y_i - \\hat f (x_i))^2 ]$\n",
    "\n",
    "For any j, s pair, the inner minimization is solved by\n",
    "\n",
    "$\\hat c_1 = \\bar R_1$, $\\hat c_2 = \\bar R_2$ as showed above.\n",
    "\n",
    "Each j, s pair can be done very quickly and hence by scanning all the possible j, s pair is feasible. Then this process is repeated until\n",
    "stopping criteria met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class DecisionTreeNode:\n",
    "\n",
    "    def __init__(self, split_point=None, split_feature=None, gini_index=None,\n",
    "                 num_instance=None, left_child=None, right_child=None, prediction=None):\n",
    "\n",
    "        self.split_point = split_point\n",
    "        self.split_feature = split_feature\n",
    "        self.gini_index = gini_index\n",
    "        self.num_instance = num_instance\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.prediction = prediction\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "\n",
    "    def __init__(self, max_depth=10, criterion='gini_index', min_sample_leave=1):\n",
    "\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.min_sample_splits = min_sample_leave\n",
    "        self._tree = None\n",
    "        self._num_features = None\n",
    "        self.feature_names = None\n",
    "        self._num_samples = None\n",
    "        self._classes = None\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "\n",
    "        self._num_samples, self._num_features = x_train.shape\n",
    "        self._classes = np.unique(y_train)\n",
    "\n",
    "        if isinstance(x_train, pd.DataFrame):\n",
    "            self.feature_names = x_train.columns\n",
    "            x_train = x_train.values\n",
    "\n",
    "        else:\n",
    "            self.feature_names = range(self._num_features)\n",
    "\n",
    "        self._tree = self._grow_tree(np.column_stack([x_train, y_train]))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _grow_tree(self, train, curr_depth=0, stop=False):\n",
    "\n",
    "        if stop:\n",
    "\n",
    "            return None\n",
    "\n",
    "        else:\n",
    "\n",
    "            impurity_dict = {}\n",
    "\n",
    "            for j in range(self._num_features):\n",
    "                curr_feature_col = train[:, j]\n",
    "\n",
    "                for s in np.unique(curr_feature_col):\n",
    "                    impurity = self._cal_impurity(train, j, s)\n",
    "\n",
    "                    if impurity in impurity_dict.keys():\n",
    "                        impurity_dict[impurity].append((j, s))\n",
    "\n",
    "                    else:\n",
    "                        impurity_dict[impurity] = [(j, s)]\n",
    "\n",
    "            min_impurity = min(impurity_dict.keys())\n",
    "            j_hat, s_hat = impurity_dict[min_impurity][0]\n",
    "            prediction = Counter(train[:, -1]).most_common()[0][0]\n",
    "            R_l = train[train[:, j_hat] < s_hat]\n",
    "            R_r = train[train[:, j_hat] >= s_hat]\n",
    "            sample_split = [len(R_l), len(R_r)]\n",
    "\n",
    "            if_stop = self._check_stopping_criterion(curr_depth, sample_split)\n",
    "\n",
    "            print(curr_depth, min_impurity, sample_split, (j_hat, s_hat))\n",
    "\n",
    "            return DecisionTreeNode(split_point=s_hat,\n",
    "                                    split_feature=j_hat,\n",
    "                                    prediction=prediction,\n",
    "                                    gini_index=min_impurity,\n",
    "                                    num_instance=len(train),\n",
    "                                    left_child=self._grow_tree(train=R_l,\n",
    "                                                               curr_depth=curr_depth+1,\n",
    "                                                               stop=if_stop),\n",
    "\n",
    "                                    right_child=self._grow_tree(train=R_r,\n",
    "                                                                curr_depth=curr_depth+1,\n",
    "                                                                stop=if_stop))\n",
    "\n",
    "    def _check_stopping_criterion(self, curr_depth, sample_split):\n",
    "\n",
    "        if curr_depth > self.max_depth:\n",
    "\n",
    "            return True\n",
    "\n",
    "        if any([sample_split[0] < self.min_sample_splits, sample_split[1] < self.min_sample_splits]):\n",
    "\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _cal_impurity(self, train, j, s):\n",
    "\n",
    "        R_l = train[train[:, j] < s]\n",
    "        R_r = train[train[:, j] >= s]\n",
    "        R_l_y = R_l[:, -1]\n",
    "        R_r_y = R_r[:, -1]\n",
    "        N_l = len(R_l) + 1e-10\n",
    "        N_r = len(R_r) + 1e-10\n",
    "\n",
    "\n",
    "        if self.criterion == 'gini_index':\n",
    "\n",
    "            gini_l = 0\n",
    "            gini_r = 0\n",
    "\n",
    "            for k in self._classes:\n",
    "                p_l = len(R_l_y[R_l_y == k]) / N_l\n",
    "                p_r = len(R_r_y[R_r_y == k]) / N_r\n",
    "                gini_l += p_l * (1 - p_l)\n",
    "                gini_r += p_r * (1 - p_r)\n",
    "\n",
    "            return gini_l * N_l + gini_r * N_r\n",
    "\n",
    "    def predict(self, x_test):\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for i in x_test:\n",
    "            output.append(self._traverse_tree(i))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def _traverse_tree(self, x, tree=None):\n",
    "\n",
    "        if not tree:\n",
    "            tree = self._tree\n",
    "\n",
    "        if not tree.left_child and not tree.left_child:\n",
    "            return tree.prediction\n",
    "\n",
    "        else:\n",
    "\n",
    "            if x[tree.split_feature] < tree.split_point:\n",
    "                return self._traverse_tree(x, tree.left_child)\n",
    "\n",
    "            else:\n",
    "                return self._traverse_tree(x, tree.right_child)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "x = load_iris()['data']\n",
    "y = load_iris()['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 50.00000000015 [50, 100] (2, 3.0)\n",
      "1 1.0000333894311098e-10 [0, 50] (0, 4.3)\n",
      "1 11.030595813383455 [54, 46] (3, 1.8)\n",
      "2 4.625000000151477 [48, 6] (2, 5.0)\n",
      "3 2.000039023286604e-10 [47, 1] (3, 1.7)\n",
      "4 1.0000389405462329e-10 [0, 47] (0, 4.9)\n",
      "4 1.000000082740371e-10 [0, 1] (0, 4.9)\n",
      "3 1.333333333488889 [3, 3] (3, 1.6)\n",
      "4 1.000000082740371e-10 [0, 3] (0, 6.0)\n",
      "4 2.000000165480742e-10 [2, 1] (0, 7.2)\n",
      "5 1.000000082740371e-10 [0, 2] (0, 6.0)\n",
      "5 1.000000082740371e-10 [0, 1] (0, 7.2)\n",
      "2 1.3333333334888893 [3, 43] (2, 4.9)\n",
      "3 2.000000165480742e-10 [1, 2] (0, 6.0)\n",
      "4 1.000000082740371e-10 [0, 1] (0, 5.9)\n",
      "4 1.000000082740371e-10 [0, 2] (0, 6.0)\n",
      "3 1.0000011929633958e-10 [0, 43] (0, 5.6)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.DecisionTreeClassifier at 0x285ff002508>"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 0.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0,\n 2.0]"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.predict(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],\n       [5.4, 3.7, 1.5, 0.2],\n       [4.8, 3.4, 1.6, 0.2],\n       [4.8, 3. , 1.4, 0.1],\n       [5.8, 4. , 1.2, 0.2],\n       [5.7, 4.4, 1.5, 0.4],\n       [5.4, 3.9, 1.3, 0.4],\n       [5.1, 3.5, 1.4, 0.3],\n       [5.7, 3.8, 1.7, 0.3],\n       [5.1, 3.8, 1.5, 0.3],\n       [5.4, 3.4, 1.7, 0.2],\n       [5.1, 3.7, 1.5, 0.4],\n       [4.6, 3.6, 1. , 0.2],\n       [5.1, 3.3, 1.7, 0.5],\n       [4.8, 3.4, 1.9, 0.2],\n       [5. , 3. , 1.6, 0.2],\n       [5. , 3.4, 1.6, 0.4],\n       [5.2, 3.5, 1.5, 0.2],\n       [5.2, 3.4, 1.4, 0.2],\n       [4.7, 3.2, 1.6, 0.2],\n       [4.8, 3.1, 1.6, 0.2],\n       [5.4, 3.4, 1.5, 0.4],\n       [5.2, 4.1, 1.5, 0.1],\n       [5.5, 4.2, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.2],\n       [5. , 3.2, 1.2, 0.2],\n       [5.5, 3.5, 1.3, 0.2],\n       [4.9, 3.6, 1.4, 0.1],\n       [4.4, 3. , 1.3, 0.2],\n       [5.1, 3.4, 1.5, 0.2],\n       [5. , 3.5, 1.3, 0.3],\n       [4.5, 2.3, 1.3, 0.3],\n       [4.4, 3.2, 1.3, 0.2],\n       [5. , 3.5, 1.6, 0.6],\n       [5.1, 3.8, 1.9, 0.4],\n       [4.8, 3. , 1.4, 0.3],\n       [5.1, 3.8, 1.6, 0.2],\n       [4.6, 3.2, 1.4, 0.2],\n       [5.3, 3.7, 1.5, 0.2],\n       [5. , 3.3, 1.4, 0.2],\n       [7. , 3.2, 4.7, 1.4],\n       [6.4, 3.2, 4.5, 1.5],\n       [6.9, 3.1, 4.9, 1.5],\n       [5.5, 2.3, 4. , 1.3],\n       [6.5, 2.8, 4.6, 1.5],\n       [5.7, 2.8, 4.5, 1.3],\n       [6.3, 3.3, 4.7, 1.6],\n       [4.9, 2.4, 3.3, 1. ],\n       [6.6, 2.9, 4.6, 1.3],\n       [5.2, 2.7, 3.9, 1.4],\n       [5. , 2. , 3.5, 1. ],\n       [5.9, 3. , 4.2, 1.5],\n       [6. , 2.2, 4. , 1. ],\n       [6.1, 2.9, 4.7, 1.4],\n       [5.6, 2.9, 3.6, 1.3],\n       [6.7, 3.1, 4.4, 1.4],\n       [5.6, 3. , 4.5, 1.5],\n       [5.8, 2.7, 4.1, 1. ],\n       [6.2, 2.2, 4.5, 1.5],\n       [5.6, 2.5, 3.9, 1.1],\n       [5.9, 3.2, 4.8, 1.8],\n       [6.1, 2.8, 4. , 1.3],\n       [6.3, 2.5, 4.9, 1.5],\n       [6.1, 2.8, 4.7, 1.2],\n       [6.4, 2.9, 4.3, 1.3],\n       [6.6, 3. , 4.4, 1.4],\n       [6.8, 2.8, 4.8, 1.4],\n       [6.7, 3. , 5. , 1.7],\n       [6. , 2.9, 4.5, 1.5],\n       [5.7, 2.6, 3.5, 1. ],\n       [5.5, 2.4, 3.8, 1.1],\n       [5.5, 2.4, 3.7, 1. ],\n       [5.8, 2.7, 3.9, 1.2],\n       [6. , 2.7, 5.1, 1.6],\n       [5.4, 3. , 4.5, 1.5],\n       [6. , 3.4, 4.5, 1.6],\n       [6.7, 3.1, 4.7, 1.5],\n       [6.3, 2.3, 4.4, 1.3],\n       [5.6, 3. , 4.1, 1.3],\n       [5.5, 2.5, 4. , 1.3],\n       [5.5, 2.6, 4.4, 1.2],\n       [6.1, 3. , 4.6, 1.4],\n       [5.8, 2.6, 4. , 1.2],\n       [5. , 2.3, 3.3, 1. ],\n       [5.6, 2.7, 4.2, 1.3],\n       [5.7, 3. , 4.2, 1.2],\n       [5.7, 2.9, 4.2, 1.3],\n       [6.2, 2.9, 4.3, 1.3],\n       [5.1, 2.5, 3. , 1.1],\n       [5.7, 2.8, 4.1, 1.3],\n       [6.3, 3.3, 6. , 2.5],\n       [5.8, 2.7, 5.1, 1.9],\n       [7.1, 3. , 5.9, 2.1],\n       [6.3, 2.9, 5.6, 1.8],\n       [6.5, 3. , 5.8, 2.2],\n       [7.6, 3. , 6.6, 2.1],\n       [4.9, 2.5, 4.5, 1.7],\n       [7.3, 2.9, 6.3, 1.8],\n       [6.7, 2.5, 5.8, 1.8],\n       [7.2, 3.6, 6.1, 2.5],\n       [6.5, 3.2, 5.1, 2. ],\n       [6.4, 2.7, 5.3, 1.9],\n       [6.8, 3. , 5.5, 2.1],\n       [5.7, 2.5, 5. , 2. ],\n       [5.8, 2.8, 5.1, 2.4],\n       [6.4, 3.2, 5.3, 2.3],\n       [6.5, 3. , 5.5, 1.8],\n       [7.7, 3.8, 6.7, 2.2],\n       [7.7, 2.6, 6.9, 2.3],\n       [6. , 2.2, 5. , 1.5],\n       [6.9, 3.2, 5.7, 2.3],\n       [5.6, 2.8, 4.9, 2. ],\n       [7.7, 2.8, 6.7, 2. ],\n       [6.3, 2.7, 4.9, 1.8],\n       [6.7, 3.3, 5.7, 2.1],\n       [7.2, 3.2, 6. , 1.8],\n       [6.2, 2.8, 4.8, 1.8],\n       [6.1, 3. , 4.9, 1.8],\n       [6.4, 2.8, 5.6, 2.1],\n       [7.2, 3. , 5.8, 1.6],\n       [7.4, 2.8, 6.1, 1.9],\n       [7.9, 3.8, 6.4, 2. ],\n       [6.4, 2.8, 5.6, 2.2],\n       [6.3, 2.8, 5.1, 1.5],\n       [6.1, 2.6, 5.6, 1.4],\n       [7.7, 3. , 6.1, 2.3],\n       [6.3, 3.4, 5.6, 2.4],\n       [6.4, 3.1, 5.5, 1.8],\n       [6. , 3. , 4.8, 1.8],\n       [6.9, 3.1, 5.4, 2.1],\n       [6.7, 3.1, 5.6, 2.4],\n       [6.9, 3.1, 5.1, 2.3],\n       [5.8, 2.7, 5.1, 1.9],\n       [6.8, 3.2, 5.9, 2.3],\n       [6.7, 3.3, 5.7, 2.5],\n       [6.7, 3. , 5.2, 2.3],\n       [6.3, 2.5, 5. , 1.9],\n       [6.5, 3. , 5.2, 2. ],\n       [6.2, 3.4, 5.4, 2.3],\n       [5.9, 3. , 5.1, 1.8]])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[x[:, 0] > 4.3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as dtc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "sklearn_dt = dtc(criterion='gini')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "a = sklearn_dt.fit(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "x = np.random.randn(150, 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 95.50827423179226 [141, 9] (2, 1.5124405135712635)\n",
      "1 89.95726495734502 [24, 117] (2, -0.9091603853372097)\n",
      "2 8.800000000141335 [15, 9] (12, 0.24230862804764847)\n",
      "3 6.200000000138 [10, 5] (1, 0.7974179820174572)\n",
      "4 2.857142857302041 [3, 7] (7, -0.9508707242673596)\n",
      "5 1.000000082740371e-10 [0, 3] (0, -1.2723087876537915)\n",
      "5 2.000000165480742e-10 [2, 5] (15, -0.5560257755504455)\n",
      "6 1.000000082740371e-10 [0, 2] (0, -0.4056771007718066)\n",
      "6 1.000000082740371e-10 [0, 5] (0, -0.5602849539531976)\n",
      "4 1.000000082740371e-10 [0, 5] (0, -1.6302814629478992)\n",
      "3 1.000000082740371e-10 [0, 9] (0, -1.1840128923662574)\n",
      "2 72.90178571437612 [96, 21] (9, 1.1895231209830655)\n",
      "3 60.047619047715415 [84, 12] (17, 0.97065890806597)\n",
      "4 50.65789473697546 [76, 8] (14, 1.4806081327956586)\n",
      "5 47.454545454624025 [54, 22] (15, 0.8265259881411527)\n",
      "6 31.836734694012577 [49, 5] (4, 1.6549885831156617)\n",
      "7 27.969696969784422 [22, 27] (12, 0.04786008006349329)\n",
      "8 7.107142857263967 [14, 8] (6, 0.7523571341720127)\n",
      "9 1.9999979450346927e-10 [1, 13] (15, -1.8753238978694915)\n",
      "9 2.400000000152 [3, 5] (1, -0.4051253328731775)\n",
      "10 1.000000082740371e-10 [0, 3] (0, -1.3004660508649324)\n",
      "10 2.000000165480742e-10 [2, 3] (4, 0.15788680773631977)\n",
      "11 1.000000082740371e-10 [0, 2] (0, -0.8826015298020558)\n",
      "11 1.000000082740371e-10 [0, 3] (0, -0.1674673330614231)\n",
      "8 13.855263158009148 [8, 19] (5, -0.2606480247326907)\n",
      "9 1.9999979450346927e-10 [7, 1] (7, 0.5651330935226334)\n",
      "9 9.125000000142968 [16, 3] (9, 0.4399377807750949)\n",
      "10 6.000000000125 [8, 8] (10, 0.0963583539354617)\n",
      "11 1.9999979450346927e-10 [7, 1] (1, 0.7499787275670741)\n",
      "11 2.5000000001375 [4, 4] (14, 0.5754463090106894)\n",
      "10 1.000000082740371e-10 [0, 3] (0, -0.6222297910688901)\n",
      "7 1.000000082740371e-10 [0, 5] (0, -1.761053690037989)\n",
      "6 6.895238095366004 [7, 15] (16, -0.1998320782504345)\n",
      "7 2.000000165480742e-10 [3, 4] (15, 1.1353439234619556)\n",
      "8 1.000000082740371e-10 [0, 3] (0, -0.27581148023092955)\n",
      "8 1.000000082740371e-10 [0, 4] (0, -0.37693502475615787)\n",
      "7 1.8571428573295918 [14, 1] (0, 2.358267897453405)\n",
      "5 1.000000082740371e-10 [0, 8] (0, -0.6568449391957085)\n",
      "4 1.8000000001819998 [2, 10] (4, -0.9396944020743295)\n",
      "5 1.000000082740371e-10 [0, 2] (0, -1.1772130089099675)\n",
      "5 2.000000165480742e-10 [9, 1] (5, 1.7779612173098938)\n",
      "3 5.22222222239321 [18, 3] (5, 1.2960408578096771)\n",
      "4 2.5000000001375 [14, 4] (15, 0.8523541710117172)\n",
      "5 9.999978622943217e-11 [0, 14] (0, -2.23027406982998)\n",
      "5 1.00000000015 [2, 2] (3, -0.016411033227653048)\n",
      "6 1.000000082740371e-10 [0, 2] (0, -0.7918658671190267)\n",
      "6 2.000000165480742e-10 [1, 1] (0, 0.42501173883541526)\n",
      "4 1.000000082740371e-10 [0, 3] (0, -0.09964752136829738)\n",
      "1 2.000000165480742e-10 [8, 1] (0, 1.094152367847867)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<__main__.DecisionTreeClassifier at 0x285c07c5148>"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.predict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dt.predict(x), y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}