{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes for text classification problem:\n",
    "\n",
    "Let $X_1, X_2, ..., X_d$ be the tokens in a text t. Let $c = \\{c_1, c_2, ..., c_k\\}$ be the set of classes.\n",
    "\n",
    "The goal is to compute the probability of text t being in class c as follows:\n",
    "\n",
    "$p(c | t) = \\frac{p(c)p(t | c)}{p(t)}$ (i.e bayes rule)\n",
    "\n",
    "Since the $p(t)$ is the same for different classes, it is a constant, that is,\n",
    "\n",
    "$\\implies p(c | t) \\propto  p(c)p(t | c) = p(c)p(X_1=x_1, X_2=x_2, ..., X_{t_d}=x_{t_d} | c)$\n",
    "\n",
    "Here, we make 2 assumptions:\n",
    "1. Given the class, the features(tokens) are independent, that is $p(X_i=x_i, X_j=x_j| c) = p(X_i=x_i | c)p(X_j=x_j | c)$\n",
    "2. Positional Independence $P(X_1=x_1 |c) = P(X_2=x_1 | c)$\n",
    "\n",
    "$\\implies p(c | t) \\propto p(c)\\prod_{1 \\leq i \\leq t_d}p(X_i = x_i |c) $\n",
    "$\\implies \\text{posteriori} \\propto \\text{prior} \\times \\text{likelihood}$\n",
    "\n",
    "We can apply MAP to find the class c.\n",
    "\n",
    "Since multiply a lot of probabilities can result in floating point underflow and log function is a monotonic function the class\n",
    "with highest probability does not change.\n",
    "\n",
    "That is we apply log functions to the posterior:\n",
    "\n",
    "$\\implies c_{map} = \\text{argmax}_{c} [\\log (p(c)) + \\sum_{i=1}^{t_d}log(p(X_i=x_i | c))]$\n",
    "\n",
    "Where,\n",
    "1. $t_d$ is the number of tokens in text t\n",
    "2. $p(X_i=x_i | c)$ follows a multinomial distribution\n",
    "3. $p(c) = \\frac{NT_{c}}{NT}$, $NT_{c}$ is number of samples in class c, $NT$ is total number of samples in dataset\n",
    "\n",
    "$\\implies \\theta_{ci} = \\hat{p}(X_i=x_i|c) = \\frac{N_{ci} + \\alpha}{N_c + \\alpha N_d}$\n",
    "\n",
    "Where,\n",
    "1. $N_{ci}$ is the number of times feature i appears in class c\n",
    "2. $N_{c}$ is the total counts of all features in class c\n",
    "3. $N_d$ is the feature size\n",
    "4. $\\alpha$ is used to prevent zero probability, when $\\alpha = 1$ is called laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.theta = None\n",
    "        self.class_prior = None\n",
    "        self.class_map = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        c_ = np.unique(y)\n",
    "        n_c = c_.size\n",
    "        NT, N_d = X.shape\n",
    "\n",
    "        self.theta = np.zeros((n_c, N_d))\n",
    "        self.class_prior = np.array(np.unique(y, return_counts=True), dtype=np.float64).T\n",
    "        self.class_prior[:, 1] = self.class_prior[:, 1] / NT\n",
    "        self.class_map = {}\n",
    "\n",
    "        for index, c in enumerate(c_):\n",
    "\n",
    "            N_c = X[y == c].sum()\n",
    "            self.class_map[index] = c\n",
    "\n",
    "            for i in range(N_d):\n",
    "\n",
    "                N_ci = X[y == c, i].sum()\n",
    "                self.theta[index, i] = np.log((N_ci + self.alpha) / (N_c + self.alpha * N_d))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        pred_results = np.zeros(X.shape[0])\n",
    "        class_size = len(self.class_map)\n",
    "\n",
    "        for index, i in enumerate(X):\n",
    "\n",
    "            temp_lst = np.zeros(class_size)\n",
    "\n",
    "            for c in range(class_size):\n",
    "\n",
    "                temp_lst[c] = np.log(self.class_prior[:, 1][c])\n",
    "\n",
    "                for d in range(X.shape[1]):\n",
    "\n",
    "                    temp_lst[c] = temp_lst[c] + self.theta[c][d] * i[d]\n",
    "\n",
    "            pred_results[index] = self.class_map[np.argmax(temp_lst)]\n",
    "\n",
    "        return pred_results\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "a = np.array([[1, 0, 1], [0, 1, 1], [0, 1, 1]])\n",
    "y = np.array([1, 2, 2])\n",
    "clf = NaiveBayes().fit(a, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X = load_iris()['data']\n",
    "y = load_iris()['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       2., 1., 2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 1., 2., 1., 2., 2.,\n       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "clf = NaiveBayes().fit(X, y)\n",
    "skl_clf = MultinomialNB().fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9533333333333334"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(skl_clf.predict(X), y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9533333333333334"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(clf.predict(X), y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.09861229, -1.09861229, -1.09861229])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}