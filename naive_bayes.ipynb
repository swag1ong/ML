{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes for text classification problem:\n",
    "\n",
    "Let $\\vec{X} = [X_1, X_2, ..., X_d]$ be the frequency of tokens in a text t. Let $c = \\{c_1, c_2, ..., c_k\\}$ be the set of classes.\n",
    "\n",
    "The goal is to compute the probability of $\\vec{X}$ being in class $c_i$ as follows:\n",
    "\n",
    "$p(c_i | \\vec{X}) = \\frac{p(c_i)p(\\vec{X} | c_i)}{p(\\vec{X})}$ (i.e bayes rule)\n",
    "\n",
    "Since the $p(\\vec{X})$ is the same for different classes, it is a constant, that is,\n",
    "\n",
    "$\\implies p(c_i | \\vec{X}) \\propto  p(c_i)p(\\vec{X} | c_i) = p(c_i)p(X_1=x_1, X_2=x_2, ..., X_{d}=x_{d} | c_i)$\n",
    "\n",
    "Here, we make 2 assumptions:\n",
    "1. Given the class, the features(tokens) are independent, that is $p(X_i=x_i, X_j=x_j| c) = p(X_i=x_i | c)p(X_j=x_j | c)$\n",
    "2. Positional Independence $P(X_1=x_1 |c) = P(X_2=x_1 | c)$\n",
    "\n",
    "$\\implies p(c_i | \\vec{X}) \\propto p(c_i)\\prod_{1 \\leq i \\leq d}p(X_i = x_i |c_i)$\n",
    "\n",
    "This is the naive bayes classification rule, if we are only interested in classification, we can classify a new sample point\n",
    "$\\vec{X^{new}}$ through $\\text{argmax}_{c} p(c | \\vec{X^{new}})$\n",
    "\n",
    "Since multiply a lot of probabilities can result in floating point underflow and log function is a monotonic function the class\n",
    "with highest probability does not change.\n",
    "\n",
    "$\\implies c = \\text{argmax}_{c} log(p(c | \\vec{X^{new}}))$\n",
    "\n",
    "Assume now we observed N sample pairs $(\\vec{X_j}, c_j)$, we can then use the MAP or MLE to estimate the parameters $\\theta$ of the model:\n",
    "\n",
    "$\\theta_{MLE} = \\text{argmax}_{\\theta} log(\\prod_{j}^{N} \\prod_{1 \\leq i \\leq d}p(X_{ji} = x_{ji} |c_j))$\n",
    "\n",
    "$\\theta_{MAP} = \\text{argmax}_{\\theta} log(\\prod_{j}^{N} p(c_j) \\prod_{1 \\leq i \\leq d}p(X_{ji} = x_{ji} |c_j))$\n",
    "\n",
    "\n",
    "Then, by assuming a multinomial distribution for $p(X_i=x_i | c)$\n",
    "\n",
    "$\\implies \\theta_{ci} = \\frac{N_{ci} + \\alpha}{N_c + \\alpha N_d}$\n",
    "\n",
    "$\\implies p(c) = \\frac{NT_{c}}{NT}$, $NT_{c}$ is number of samples in class c, $NT$ is total number of samples in dataset\n",
    "\n",
    "\n",
    "Where,\n",
    "1. $N_{ci}$ is the number of times feature i appears in class c\n",
    "2. $N_{c}$ is the total counts of all features in class c\n",
    "3. $N_d$ is the feature size\n",
    "4. $\\alpha$ is used to prevent zero probability, when $\\alpha = 1$ is called laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "\n",
    "    def __init__(self, alpha=1):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.theta = None\n",
    "        self.class_prior = None\n",
    "        self.class_map = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        c_ = np.unique(y)\n",
    "        n_c = c_.size\n",
    "        NT, N_d = X.shape\n",
    "\n",
    "        self.theta = np.zeros((n_c, N_d))\n",
    "        self.class_prior = np.array(np.unique(y, return_counts=True), dtype=np.float64).T\n",
    "        self.class_prior[:, 1] = self.class_prior[:, 1] / NT\n",
    "        self.class_map = {}\n",
    "\n",
    "        for index, c in enumerate(c_):\n",
    "\n",
    "            N_c = X[y == c].sum()\n",
    "            self.class_map[index] = c\n",
    "\n",
    "            for i in range(N_d):\n",
    "\n",
    "                N_ci = X[y == c, i].sum()\n",
    "                self.theta[index, i] = np.log((N_ci + self.alpha) / (N_c + self.alpha * N_d))\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "\n",
    "        pred_results = np.zeros(X.shape[0])\n",
    "        class_size = len(self.class_map)\n",
    "\n",
    "        for index, i in enumerate(X):\n",
    "\n",
    "            temp_lst = np.zeros(class_size)\n",
    "\n",
    "            for c in range(class_size):\n",
    "\n",
    "                temp_lst[c] = np.log(self.class_prior[:, 1][c])\n",
    "\n",
    "                for d in range(X.shape[1]):\n",
    "\n",
    "                    temp_lst[c] = temp_lst[c] + self.theta[c][d] * i[d]\n",
    "\n",
    "            pred_results[index] = self.class_map[np.argmax(temp_lst)]\n",
    "\n",
    "        return pred_results\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "a = np.array([[1, 0, 1], [0, 1, 1], [0, 1, 1]])\n",
    "y = np.array([1, 2, 2])\n",
    "clf = NaiveBayes().fit(a, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X = load_iris()['data']\n",
    "y = load_iris()['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       2., 1., 2., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 2.,\n       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 1., 2., 1., 2., 2.,\n       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "clf = NaiveBayes().fit(X, y)\n",
    "skl_clf = MultinomialNB().fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9533333333333334"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(skl_clf.predict(X), y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9533333333333334"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(clf.predict(X), y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.09861229, -1.09861229, -1.09861229])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}