{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SVM\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "Given $X = \\{x_1, x_2, ...., x_n\\} \\in R^m$\n",
    "\n",
    "Let $w^Tx + b = 0$ be a hyperplane, we classify a new point $x_i$ by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "      w^Tx_i + b >= 0, & y_i = 1\\\\\n",
    "      w^Tx_i + b < 0, & y_i = -1\\\\\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Where $y = \\{1, -1\\}$ is the class.\n",
    "\n",
    "Then we can rewrite this classification rule into one formula $y_i(w^Tx_i +b) >= 0$\n",
    "\n",
    "### Margin\n",
    "The concept of the margin is intuitively simple: It is the distance of the\n",
    "There could be two separating hyperplane to the closest examples in the dataset assuming that the dataset is nearly\n",
    "linearly separable.\n",
    "\n",
    "Let $x_i$ be a sample that has label $y_i = 1$, thus, it is on the positive side of the hyperplane\n",
    "$w^Tx + b = 0$. Define r to be the distance between point $x_i$ and plane $w^Tx + b = 0$. Since the shortest distance from\n",
    "a point to a plane is the distance to its projection on the plane. Define $x_i^p$ to be the projection of point $x_i$ on\n",
    "plane $w^Tx + b = 0$.\n",
    "\n",
    "Since $w$ is orthogonal to the plane, and $\\frac{w}{\\|w\\|}$ corresponding to the direction of w, $r * \\frac{w}{\\|w\\|}$ is\n",
    "the vector between $x_i, x_i^p$, thus, $x_i - x_i^p = r * \\frac{w}{\\|w\\|} \\implies x_i = x_i^p +  r * \\frac{w}{\\|w\\|}$\n",
    "\n",
    "### The goal\n",
    "\n",
    "The goal is to maximize r while classify points to the correct classes. That is, we want to\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b, r}{\\text{max}}\n",
    "& & r \\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^Tx_i + b) \\geq r, r > 0\n",
    "\\end{aligned}\n",
    "\n",
    "Let $x_i$ be the point that exactly lies on the margin that has label 1, then $w^T x_i  + b = 1$ and\n",
    "$x_i^p$ is on the hyperplane,\n",
    "\n",
    "$w^T x_i^p + b = 0 \\implies w^T(x_i - r * \\frac{w}{\\|w\\|}) + b = 0$ $\\implies (w^T x_i  + b) - w^T r * \\frac{w}{\\|w\\|}$\n",
    "$\\implies w^T r*\\frac{w}{\\|w\\|} = 1$\n",
    "\n",
    "we know that $w^T w = \\|w\\|^2 \\implies r = \\frac{1}{\\|w\\|}$\n",
    "\n",
    "Therefore, we can rewrite this as (1 is because we only care about the direction of w)\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b, r}{\\text{max}}\n",
    "& &  \\frac{1}{\\|w\\|}\\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^Tx_i + b) \\geq 1\n",
    "\\end{aligned}\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{min}}\n",
    "& &  \\frac{1}{2}\\|w\\|^2\\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^Tx_i + b) \\geq 1\n",
    "\\end{aligned}\n",
    "\n",
    "### Slack variables\n",
    "At the same time, we introduce a new slack variable $\\xi_i, \\xi_i \\geq 0, \\forall i$ to allow misclassification of points(to add some regularization).get\n",
    "\n",
    "1. if $\\xi_i = 1$, the point is on the decision boundary\n",
    "2. if the point is misclassified, $\\xi_i =|y_i - \\hat y(x_i)| > 1$\n",
    "3. if $\\xi_i = 0$, the point is on the margin or outside of margin\n",
    "\n",
    "We also define a regularizer C, s.t $C> 0$ and $\\sum_{i=1}^{n} \\xi_i \\leq C$\n",
    "\n",
    "## Primal Problem\n",
    "\n",
    "Combine all the things, we can formulate our problem as:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{min}}\n",
    "& &  \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=0}^{n} \\xi_i\\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^Tx_i + b) \\geq 1 - \\xi_i,\n",
    "& & \\xi_i \\geq 0\n",
    "\\end{aligned}\n",
    "\n",
    "as $C \\rightarrow \\infty$, we have less regularization, that is, the classifier will focus more on classifying\n",
    "training set right, this may generalize badly and we get a hard margin SVM.\n",
    "\n",
    "By applying lagrange multiplier, we have our unconstrained primal problem:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{min}}\n",
    "& &  L(w, b, \\xi) = \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=0}^{n} \\xi_i - \\sum_{i=1}^{n}\\alpha_i[y_i(w^Tx_i + b) - 1 + \\xi_i] - \\sum_{i=1}^{n} \\mu_i\\xi_i\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "### Solving Primal Problem\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{min}}\n",
    "& &  L(w, b, \\xi) = \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=0}^{n} \\xi_i - \\sum_{i=1}^{n}\\alpha_i[y_i(w^Tx_i + b) - 1 + \\xi_i] - \\sum_{i=1}^{n} \\mu_i\\xi_i\\\\\n",
    "& \\frac{\\partial L(w, b, \\xi)}{\\partial w} = w - \\sum_{i=1}^{n} \\alpha_i y_i x_i = 0 \\\\\n",
    "& \\implies w^* = \\sum_{i=1}^{n} \\alpha_i y_i x_i \\\\\n",
    "& \\frac{\\partial L(w, b, \\xi)}{\\partial b} =\\sum_{i=1}^{n} \\alpha_i y_i = 0 \\\\\n",
    "& \\implies \\sum_{i=1}^{n} \\alpha_i y_i = 0 \\\\\n",
    "& \\frac{\\partial L(w, b, \\xi)}{\\partial \\xi} = C - \\alpha - \\mu = 0 \\\\\n",
    "& \\implies \\alpha = C - \\mu\n",
    "\\end{aligned}\n",
    "\n",
    "where $\\alpha = [\\alpha_1 \\alpha_2 .... \\alpha_n]^T, \\mu = [\\mu_1 \\mu_2 ..... \\mu_n]^T$\n",
    "\n",
    "Plug the above results back to the primal problem, we have:\n",
    "\n",
    "\\begin{aligned}\n",
    "L(w, b, \\xi) &= -\\frac{1}{2}\\|w\\|^2 + C\\sum_{i=0}^{n} \\xi_i - \\sum_{i=1}^{n}\\alpha_i[y_i(w^Tx_i + b) - 1 + \\xi_i] - \\sum_{i=1}^{n} \\mu_i\\xi_i\\\\\n",
    "&=\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_i x_i y_i)^T(\\sum_{i=1}^{n}\\alpha_i x_i y_i) - \\sum_{i=1}^{n} \\alpha_i(\\sum_{k=1}^{n}\\alpha_k x_k y_k)x_iy_i + \\alpha_iby_i - \\alpha_i  + (C\\sum_{i=0}^{n} \\xi_i - \\sum_{i=1}^{n}\\alpha_i \\xi_i - \\sum_{i=1}^{n} \\mu_i\\xi_i)\\\\\n",
    "&=\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{k=1}^{n}\\alpha_i \\alpha_k {x_i}^T x_k y_i y_k - \\sum_{i=1}^{n}\\sum_{k=1}^{n}\\alpha_i \\alpha_k {x_i}^T x_k y_i y_k - b\\sum_{i=1}^{n}a_iy_i + \\sum_{i=1}^{n} \\alpha_i (C\\sum_{i=0}^{n} \\xi_i - \\sum_{i=1}^{n} + (C - \\mu_i)\\xi_i - \\sum_{i=1}^{n} \\mu_i\\xi_i)\\\\\n",
    "&=- \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{k=1}^{n}\\alpha_i \\alpha_k {x^i}^T x^k y^i y^k + \\sum_{i=1}^{n} \\alpha_i\n",
    "\\end{aligned}\n",
    "\n",
    "Thus, we obtain our dual problem\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{\\alpha}{\\text{max}}\n",
    "& &  L(w^*, b^*, \\xi^*) = \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{k=1}^{n}\\alpha_i \\alpha_k {x^i}^T x^k y^i y^k + \\sum_{i=1}^{n} \\alpha_i\\\\\n",
    "& \\text{subject to}\n",
    "& & 0 \\leq \\alpha_i \\leq C \\\\\n",
    "& & & \\sum_{i=1}^{n} \\alpha_i y_i = 0 \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "$0 \\leq \\alpha_i \\leq C$ is obtained from $\\alpha_i + \\mu_i = C$ where $\\mu_i \\geq 0$\n",
    "\n",
    "By solving the above quadratic programming problem, we obtain optimal $\\alpha_i$ for each sample $x_i$.\n",
    "\n",
    "### KKT Condition\n",
    "\n",
    "The following KKT conditions are sufficient and necessary:\n",
    "\n",
    "1. $\\alpha_i, \\mu_i, \\xi_i \\geq 0$\n",
    "2. $\\alpha_i[y_i(w^Tx_i + b) - (1 - \\xi_i)] = 0$\n",
    "3. $\\mu_i \\xi_i =0$\n",
    "4. $y_i(w^Tx_i + b) - (1 - \\xi_i) \\geq 0$\n",
    "\n",
    "$\\forall i=1, ..., n$\n",
    "\n",
    "Thus, we could conclude:\n",
    "\n",
    "1. if $0 < \\alpha_i < C \\implies y_i(w^T x_i + b) = 1 - \\xi_i$ Since $\\alpha_i = C - \\mu_i, \\mu_i \\geq 0$\n",
    "from condition 3 we have $\\xi_i =0 \\implies$ the points are with $0 < \\alpha_i < C$ are on the margin\n",
    "\n",
    "2. if $\\alpha_i = C$\n",
    "    - $0 < \\xi_i < 1$: the points are inside the margin on the correct side\n",
    "    - $\\xi_i = 1$: the points are on the decision boundary\n",
    "    - $\\xi_i > 1$: the points are inside the wrong side of the margin and misclassified\n",
    "3. if $\\alpha_i = 0$, the points are not support vectors, have no affect on the weight.\n",
    "\n",
    "### Kernel\n",
    "\n",
    "Sometimes with linear classifier, we encounter some data that can not be linearly separated, one way around this is to\n",
    "introduce a feature mapping $\\phi (x)$ to our equation, transform the features to higher dimensional space,\n",
    "so that the decision boundary could be linear there.\n",
    "\n",
    "For example let $x = [x_1, x_2]^T$, a polynomial transformation could be $\\phi(x) = [x_1^2, \\sqrt{2}x_1x_2, x_2^2]$\n",
    "\n",
    "The basic idea of feature mapping $\\phi(x)$ is to create new, non-linear features based on given basis features.\n",
    "\n",
    "Thus, applying this technique, we could have non-linear decision boundary with SVM:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{min}}\n",
    "& &  \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=0}^{n} \\xi_i\\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^T \\phi(x_i) + b) \\geq 1 - \\xi_i,\n",
    "& & \\xi_i \\geq 0\n",
    "\\end{aligned}\n",
    "\n",
    "by solving for w and b we can make prediction for new x by:\n",
    "\n",
    "$\\hat y = \\sum_{i=1}^{n} \\alpha_i y_i \\phi(x)^T\\phi(x_i) + b$\n",
    "\n",
    "Where $\\phi(x)$ is the feature mapping and could be very hard to compute\n",
    "\n",
    "Instead of compute $\\phi(x)^T\\phi(x_i)$ we could replace this with kernel function $k(x, x_i) = \\phi(x)^T\\phi(x_i) = \\sum_{j=1}^{n} \\phi_j(x) \\phi_j(x_i)$\n",
    "\n",
    "we could apply kernel functions on the basis features of x and $x_i$ to compute the value $\\phi(x)^T\\phi(x_i)$ without directly\n",
    "compute the $\\phi(x)^T\\phi(x_i)$\n",
    "\n",
    "For example $k(x, z) = (x^Tz)^2 = (x_1z_1 + x_2 z_2)^2 = (x_1^2z_1^2 + 2x_1z_1x_2z_2 + x_2^2z_2^2)^2 = [x_1^2, \\sqrt{2}x_1x_2, x_2^2]^T [z_1^2, \\sqrt{2}z_1z_2, z_2^2] = \\phi(x)^T\\phi(z)$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$\\hat y = \\sum_{i=1}^{n} \\alpha_i y_i k(x, x_i) + b$\n",
    "\n",
    "\n",
    "### Solve for b\n",
    "\n",
    "We notice that those points with $0 < \\alpha_i < C$ lies exactly on the margin, that is, let $x_i$ be one of these points\n",
    "\n",
    "$y_i[w^Tx_i + b] = 1 \\implies y_i[\\alpha_i y_i k(x, x_i) + b] = 1$\n",
    "\n",
    "Let M be a set of points that lies exactly on the margin, a more stable solution is obtained by averaging over all points\n",
    "\n",
    "$b = \\frac{1}{N_{m}} \\sum_{i=1}^{N_{m}} (y_i - \\sum_{j=1}^{N_{m}} \\alpha_j y_j k(x_i, x_j))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "import numpy as np\n",
    "from qpsolvers import solve_qp\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class SVM:\n",
    "\n",
    "    def __init__(self, c=1, kernel='linear'):\n",
    "\n",
    "        self.c = c\n",
    "        self.kernel = kernel\n",
    "        self.b = None\n",
    "        self.dual_coef_ = None\n",
    "        self.decision_matrix = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n, d = X.shape\n",
    "        y = y.reshape(-1, 1)\n",
    "        yyt = np.matmul(y, y.T)\n",
    "        P = np.zeros((n, n))\n",
    "        q = matrix(-np.ones((n, 1)))\n",
    "        a = matrix(y.T, tc='d')\n",
    "        b = matrix([0.0])\n",
    "        G = matrix(np.row_stack([np.diag([-1] * n), np.diag([1] * n)]), tc='d')\n",
    "        h = matrix(np.row_stack([np.array([0] * n).reshape(n, 1),\n",
    "                                 np.array([self.c] * n).reshape(n, 1)]), tc='d')\n",
    "        for i in range(n):\n",
    "\n",
    "            for j in range(n):\n",
    "\n",
    "                P[i][j] = self.apply_kernel(X[i], X[j])\n",
    "\n",
    "        P = matrix(P * yyt)\n",
    "        alpha = np.array(solvers.qp(P, q, G, h , a, b)['x'])\n",
    "        alpha[alpha < np.mean(alpha) * 0.1] = 0\n",
    "        temp_x = np.column_stack([X, alpha, y])\n",
    "        m = temp_x[(temp_x[:, -2] > 0) & (temp_x[:, -2] < self.c)]\n",
    "        N_m = len(m)\n",
    "        self.decision_matrix = m[:, :-2]\n",
    "        self.b = 0\n",
    "        self.dual_coef_ = m[:, -1] * m[:, -2]\n",
    "\n",
    "        ## get b\n",
    "        for i in range(N_m):\n",
    "\n",
    "            self.b += m[i, -1]\n",
    "\n",
    "            for j in range(N_m):\n",
    "\n",
    "                self.b -= m[j, -2] * m[j, -1] * self.apply_kernel(m[i, :-2], m[j, :-2])\n",
    "\n",
    "        self.b = self.b / N_m\n",
    "\n",
    "        return self\n",
    "\n",
    "    def apply_kernel(self, x_1, x_2):\n",
    "\n",
    "        if self.kernel == 'linear':\n",
    "            return np.dot(x_1, x_2)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "\n",
    "        pred_results = np.array([])\n",
    "\n",
    "        for i in range(len(X)):\n",
    "\n",
    "            pred = self.b\n",
    "\n",
    "            for j in range(len(self.decision_matrix)):\n",
    "\n",
    "                pred += self.dual_coef_[j] * self.apply_kernel(X[i], self.decision_matrix[j])\n",
    "\n",
    "            pred_results = np.append(pred_results, pred)\n",
    "\n",
    "        return pred_results\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        pred_results = self.decision_function(X)\n",
    "\n",
    "        return np.where(pred_results >= 0, 1, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the alphas for clf is : [-0.67133312 -0.07672382  0.74805781]\n",
      "the alphas for sklearn_clf is : [[-0.67075289 -0.07709756  0.74785045]]\n",
      "the b for clf is : -1.4505932766999716\n",
      "the b for sklearn_clf is : [-1.4528445]\n",
      "the acc for clf is : 1.0\n",
      "the acc for sklearn_clf is : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "x = load_iris()['data'][:100]\n",
    "y = load_iris()['target'][:100]\n",
    "y = np.where(y == 0, -1, y)\n",
    "clf = SVM().fit(x, y)\n",
    "sklearn_clf = SVC(kernel='linear').fit(x, y)\n",
    "\n",
    "print(f'the alphas for clf is : {clf.dual_coef_}')\n",
    "print(f'the alphas for sklearn_clf is : {sklearn_clf.dual_coef_}')\n",
    "\n",
    "print(f'the b for clf is : {clf.b}')\n",
    "print(f'the b for sklearn_clf is : {sklearn_clf.intercept_}')\n",
    "\n",
    "print(f'the acc for clf is : {accuracy_score(y, clf.predict(x))}')\n",
    "print(f'the acc for sklearn_clf is : {accuracy_score(y, sklearn_clf.predict(x))}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}