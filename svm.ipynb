{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SVM\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "Given $X = \\{x_1, x_2, ...., x_n\\} \\in R^m$\n",
    "\n",
    "Let $w^Tx + b = 0$ be a hyperplane, we classify a new point $x_i$ by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{cases}\n",
    "      w^Tx_i + b >= 0, & y_i = 1\\\\\n",
    "      w^Tx_i + b < 0, & y_i = -1\\\\\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Where $y = \\{1, -1\\}$ is the class.\n",
    "\n",
    "Then we can rewrite this classification rule into one formula $y_i(w^Tx_i +b) >= 0$\n",
    "\n",
    "### Margin\n",
    "The concept of the margin is intuitively simple: It is the distance of the\n",
    "There could be two separating hyperplane to the closest examples in the dataset assuming that the dataset is nearly\n",
    "linearly separable.\n",
    "\n",
    "Let $x_i$ be a sample that has label $y_i = 1$, thus, it is on the positive side of the hyperplane\n",
    "$w^Tx + b = 0$. Define r to be the distance between point $x_i$ and plane $w^Tx + b = 0$. Since the shortest distance from\n",
    "a point to a plane is the distance to its projection on the plane. Define $x_i^p$ to be the projection of point $x_i$ on\n",
    "plane $w^Tx + b = 0$.\n",
    "\n",
    "Since $w$ is orthogonal to the plane, and $\\frac{w}{\\|w\\|}$ corresponding to the direction of w, $r * \\frac{w}{\\|w\\|}$ is\n",
    "the vector between $x_i, x_i^p$, thus, $x_i - x_i^p = r * \\frac{w}{\\|w\\|} \\implies x_i = x_i^p +  r * \\frac{w}{\\|w\\|}$\n",
    "\n",
    "### The goal\n",
    "\n",
    "The goal is to maximize r while classify points to the correct classes. That is, we want to\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b, r}{\\text{max}}\n",
    "& & r \\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^Tx_i + b) \\geq r, r > 0\n",
    "\\end{aligned}\n",
    "\n",
    "Let $x_i$ be the point that exactly lies on the margin that has label 1, then $w^T x_i  + b = 1$ and\n",
    "$x_i^p$ is on the hyperplane,\n",
    "\n",
    "$w^T x_i^p + b = 0 \\implies w^T(x_i - r * \\frac{w}{\\|w\\|}) + b = 0$ $\\implies (w^T x_i  + b) - w^T r * \\frac{w}{\\|w\\|}$\n",
    "$\\implies w^T r*\\frac{w}{\\|w\\|} = 1$\n",
    "\n",
    "we know that $w^T w = \\|w\\|^2 \\implies r = \\frac{1}{\\|w\\|}$\n",
    "\n",
    "Therefore, we can rewrite this as (1 is because we only care about the direction of w)\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b, r}{\\text{max}}\n",
    "& &  \\frac{1}{\\|w\\|}\\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^Tx_i + b) \\geq 1\n",
    "\\end{aligned}\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{min}}\n",
    "& &  \\frac{1}{2}\\|w\\|^2\\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^Tx_i + b) \\geq 1\n",
    "\\end{aligned}\n",
    "\n",
    "### Slack variables\n",
    "At the same time, we introduce a new slack variable $\\xi_i, \\xi_i \\geq 0, \\forall i$ to allow classification of points(to add some regularization).get\n",
    "\n",
    "1. if $\\xi_i = 1$, the point is on the decision boundary\n",
    "2. if the point is misclassified, $\\xi_i =|y_i - \\hat y(x_i)| > 1$\n",
    "3. if $\\xi_i = 0$, the point is on the margin or outside of margin\n",
    "\n",
    "We also define a regularizer C, s.t $C> 0$ and $\\sum_{i=1}^{n} \\xi_i \\leq C$\n",
    "\n",
    "## Primal Problem\n",
    "\n",
    "Combine all the things, we can formulate our problem as:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{min}}\n",
    "& &  \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=0}^{n} \\xi_i\\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^Tx_i + b) \\geq 1 - \\xi_i,\n",
    "& & \\xi_i \\geq 0\n",
    "\\end{aligned}\n",
    "\n",
    "as $C \\rightarrow \\infty$, we have less regularization, that is, the classifier will focus more on classifying\n",
    "training set right, this may generalize badly and we get a hard margin SVM.\n",
    "\n",
    "By applying lagrange multiplier, we have our unconstrained primal problem:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{min}}\n",
    "& &  L(w, b, \\xi) = \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=0}^{n} \\xi_i - \\sum_{i=1}^{n}\\alpha_i[y_i(w^Tx_i + b) - 1 + \\xi_i] - \\sum_{i=1}^{n} \\mu_i\\xi_i\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "### Solving Primal Problem\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{min}}\n",
    "& &  L(w, b, \\xi) = \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=0}^{n} \\xi_i - \\sum_{i=1}^{n}\\alpha_i[y_i(w^Tx_i + b) - 1 + \\xi_i] - \\sum_{i=1}^{n} \\mu_i\\xi_i\\\\\n",
    "& \\frac{\\partial L(w, b, \\xi)}{\\partial w} = w - \\sum_{i=1}^{n} \\alpha_i y_i x_i = 0 \\\\\n",
    "& \\implies w^* = \\sum_{i=1}^{n} \\alpha_i y_i x_i \\\\\n",
    "& \\frac{\\partial L(w, b, \\xi)}{\\partial b} =\\sum_{i=1}^{n} \\alpha_i y_i = 0 \\\\\n",
    "& \\implies \\sum_{i=1}^{n} \\alpha_i y_i = 0 \\\\\n",
    "& \\frac{\\partial L(w, b, \\xi)}{\\partial \\xi} = C - \\alpha - \\mu = 0 \\\\\n",
    "& \\implies \\alpha = C - \\mu\n",
    "\\end{aligned}\n",
    "\n",
    "where $\\alpha = [\\alpha_1 \\alpha_2 .... \\alpha_n]^T, \\mu = [\\mu_1 \\mu_2 ..... \\mu_n]^T$\n",
    "\n",
    "Plug the above results back to the primal problem, we have:\n",
    "\n",
    "\\begin{aligned}\n",
    "L(w, b, \\xi) &= \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=0}^{n} \\xi_i - \\sum_{i=1}^{n}\\alpha_i[y_i(w^Tx_i + b) - 1 + \\xi_i] - \\sum_{i=1}^{n} \\mu_i\\xi_i\\\\\n",
    "&=\\frac{1}{2}(\\sum_{i=1}^{n}\\alpha_i x_i y_i)^T(\\sum_{i=1}^{n}\\alpha_i x_i y_i) - \\sum_{i=1}^{n} \\alpha_i(\\sum_{k=1}^{n}\\alpha_k x_k y_k)x_iy_i + \\alpha_iby_i - \\alpha_i  + (C\\sum_{i=0}^{n} \\xi_i - \\sum_{i=1}^{n}\\alpha_i \\xi_i - \\sum_{i=1}^{n} \\mu_i\\xi_i)\\\\\n",
    "&=\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{k=1}^{n}\\alpha_i \\alpha_k {x_i}^T x_k y_i y_k - \\sum_{i=1}^{n}\\sum_{k=1}^{n}\\alpha_i \\alpha_k {x_i}^T x_k y_i y_k - b\\sum_{i=1}^{n}a_iy_i + \\sum_{i=1}^{n} \\alpha_i (C\\sum_{i=0}^{n} \\xi_i - \\sum_{i=1}^{n} + (C - \\mu_i)\\xi_i - \\sum_{i=1}^{n} \\mu_i\\xi_i)\\\\\n",
    "&=- \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{k=1}^{n}\\alpha_i \\alpha_k {x^i}^T x^k y^i y^k + \\sum_{i=1}^{n} \\alpha_i\n",
    "\\end{aligned}\n",
    "\n",
    "Thus, we obtain our dual problem\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{\\alpha}{\\text{max}}\n",
    "& &  L(w^*, b^*, \\xi^*) = \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{k=1}^{n}\\alpha_i \\alpha_k {x^i}^T x^k y^i y^k + \\sum_{i=1}^{n} \\alpha_i\\\\\n",
    "& \\text{subject to}\n",
    "& & 0 \\leq \\alpha_i \\leq C \\\\\n",
    "& & & \\sum_{i=1}^{n} \\alpha_i y_i = 0 \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "$0 \\leq \\alpha_i \\leq C$ is obtained from $\\alpha_i + \\mu_i = C$ where $\\mu_i \\geq 0$\n",
    "\n",
    "By solving the above quadratic programming problem, we obtain optimal $\\alpha_i$ for each sample $x_i$.\n",
    "\n",
    "### KKT Condition\n",
    "\n",
    "The following KKT conditions are sufficient and necessary:\n",
    "\n",
    "1. $\\alpha_i, \\mu_i, \\xi_i \\geq 0$\n",
    "2. $\\alpha_i[y_i(w^Tx_i + b) - (1 - \\xi_i)] = 0$\n",
    "3. $\\mu_i \\xi_i =0$\n",
    "4. $y_i(w^Tx_i + b) - (1 - \\xi_i) \\geq 0$\n",
    "\n",
    "$\\forall i=1, ..., n$\n",
    "\n",
    "Thus, we could conclude:\n",
    "\n",
    "1. if $0 < \\alpha_i < C \\implies y_i(w^T x_i + b) = 1 - \\xi_i$ Since $\\alpha_i = C - \\mu_i, \\mu_i \\geq 0$\n",
    "from condition 3 we have $\\xi_i =0 \\implies$ the points are with $0 < \\alpha_i < C$ are on the margin\n",
    "\n",
    "2. if $\\alpha_i = C$\n",
    "    - $0 < \\xi_i < 1$: the points are inside the margin on the correct side\n",
    "    - $\\xi_i = 1$: the points are on the decision boundary\n",
    "    - $\\xi_i > 1$: the points are inside the wrong side of the margin and misclassified\n",
    "3. if $\\alpha_i = 0$, the points are not support vectors, have no affect on the weight.\n",
    "\n",
    "### Kernel\n",
    "\n",
    "Sometimes with linear classifier, we encounter some data that can not be linearly separated, one way around this is to\n",
    "introduce a feature mapping $\\phi (x)$ to our equation, transform the features to higher dimensional space,\n",
    "so that the decision boundary could be linear there.\n",
    "\n",
    "For example let $x = [x_1, x_2]^T$, a polynomial transformation could be $\\phi(x) = [x_1^2, \\sqrt{2}x_1x_2, x_2^2]$\n",
    "\n",
    "The basic idea of feature mapping $\\phi(x)$ is to create new, non-linear features based on given basis features.\n",
    "\n",
    "Thus, applying this technique, we could have non-linear decision boundary with SVM:\n",
    "\n",
    "\\begin{aligned}\n",
    "& \\underset{w, b}{\\text{min}}\n",
    "& &  \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=0}^{n} \\xi_i\\\\\n",
    "& \\text{subject to}\n",
    "& & y_i(w^T \\phi(x_i) + b) \\geq 1 - \\xi_i,\n",
    "& & \\xi_i \\geq 0\n",
    "\\end{aligned}\n",
    "\n",
    "by solving for w and b we can make prediction for new x by:\n",
    "\n",
    "$\\hat y = \\sum_{i=1}^{n} \\alpha_i y_i \\phi(x)^T\\phi(x_i) + b$\n",
    "\n",
    "Where $\\phi(x)$ is the feature mapping and could be very hard to compute\n",
    "\n",
    "Instead of compute $\\phi(x)^T\\phi(x_i)$ we could replace this with kernel function $k(x, x_i) = \\phi(x)^T\\phi(x_i) = \\sum_{j=1}^{n} \\phi_j(x) \\phi_j(x_i)$\n",
    "\n",
    "we could apply kernel functions on the basis features of x and $x_i$ to compute the value $\\phi(x)^T\\phi(x_i)$ without directly\n",
    "compute the $\\phi(x)^T\\phi(x_i)$\n",
    "\n",
    "For example $k(x, z) = (x^Tz)^2 = (x_1z_1 + x_2 z_2)^2 = (x_1^2z_1^2 + 2x_1z_1x_2z_2 + x_2^2z_2^2)^2 = [x_1^2, \\sqrt{2}x_1x_2, x_2^2]^T [z_1^2, \\sqrt{2}z_1z_2, z_2^2] = \\phi(x)^T\\phi(z)$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$\\hat y = \\sum_{i=1}^{n} \\alpha_i y_i k(x, x_i) + b$\n",
    "\n",
    "\n",
    "### solve for b\n",
    "\n",
    "We notice that those points with $0 < \\alpha_i < C$ lies exactly on the margin, that is, let $x_i$ be one of these points\n",
    "\n",
    "$y_i[w^Tx_i + b] = 1 \\implies y_i[\\alpha_i y_i k(x, x_i) + b] = 1$\n",
    "\n",
    "Let M be a set of points that lies exactly on the margin, a more stable solution is obtained by averaging over all points\n",
    "\n",
    "$b = \\frac{1}{N_{m}} \\sum_{i=1}^{N_{m}} (y_i - \\sum_{j=1}^{n} \\alpha_j y_j k(x_i, x_j))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}