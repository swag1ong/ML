{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Perceptron \n",
    "\n",
    "*This is based on ESLII 4.5.1*\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "The perceptron learning algorithm tries to find a separating hyperplane by minimizing the distance of misclassified\n",
    "points to the decision boundary $w^T x + b = 0$\n",
    "\n",
    "When $x_i$ lies exactly on the decision boundary it will be classified as positive, in order to prevent this, we will use $f(x_i)y_i > 0$ to check if the point has been misclassified or not\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal is to minimize the objective:\n",
    "\n",
    "$D(w, b) = - \\sum_{i\\in M} y_i(w^T x_i + b)$\n",
    "\n",
    "if misclassified:\n",
    "1. $y_i =1, f(x_i) < 0 \\implies -y_i f(x_i) > 0 \\implies$ increases the cost\n",
    "2. $y_i =-1, f(x_i) > 0 \\implies -y_i f(x_i) > 0 \\implies$ increases the cost\n",
    "\n",
    "Where M is the set of all misclassified points\n",
    "\n",
    "$D(w, b)$ is non-negative and proportional to the distance of the misclassified points to the decision boundary defined by $w^T x + b = 0$\n",
    "\n",
    "## Why Proportional\n",
    "\n",
    "For any misclassified point x, let $x_0$ be its projection on $w^T x + b = 0 \\implies w^T x_0 + b = 0$ \n",
    "\n",
    "Let r be the distance from x to $w^T x + b = 0$\n",
    "\n",
    "$\\implies w^T x_0 = -b, x-x_0 = \\frac{w}{\\|w\\|} r$\n",
    "\n",
    "$\\implies w^Tx - w^T x_0 = \\frac{w^Tw}{\\|w\\|} r$\n",
    "\n",
    "$\\implies w^Tx + b = \\| w\\| r$\n",
    "\n",
    "$\\implies r = \\frac{1}{\\| w\\|} f(x)$\n",
    "\n",
    "Thus, r, the distance to the decision boundary is proportional to f(x) which is the prediction.\n",
    "\n",
    "## Update rule\n",
    "\n",
    "By taking the gradients wrt $w, b$\n",
    "\n",
    "we have:\n",
    "\n",
    "$\\frac{\\partial D(w, b)}{\\partial w} = - \\sum_{i\\in M} x_i y_i$\n",
    "\n",
    "$\\frac{\\partial D(w, b)}{\\partial b} = - \\sum_{i\\in M} y_i$\n",
    "\n",
    "Instead of taking the sum, the misclassified observations are visited in some sequence, and the parameters $w, b$ are updated via:\n",
    "\n",
    "$\\begin{pmatrix}\n",
    "w^{n+1}\\\\\n",
    "b^{n+1}\n",
    "\\end{pmatrix}\\rightarrow\n",
    "\\begin{pmatrix}\n",
    "w^{n}\\\\\n",
    "b^{n}\n",
    "\\end{pmatrix} + \\alpha\n",
    "\\begin{pmatrix}\n",
    "y_i x_i\\\\\n",
    "y_i\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "where $\\alpha$ here is the learning rate.\n",
    "\n",
    "## Problems with Perceptron\n",
    "\n",
    "1. When the data are separable, there are many solutions adn which one is found depends on the starting values.\n",
    "2. The finite number of steps can be very large. The smaller the gap, the longer the time to find it.\n",
    "3. When the data are not separable, the algorithm will not converge, and cycles develop. The cycles can be long and therefore hard to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, alpha=1, random_seed=16):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.random_seed = random_seed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}