{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The goal of logistic regression is to train a classifier that can make a decision about the class of a new input observation.py\n",
    "\n",
    "\n",
    "## Binary\n",
    "\n",
    "We begin with the sigmoid function $\\sigma(a) = \\frac{1}{1 + e^{-a}}$. This function maps a input value to a output value between\n",
    "[0, 1] which is exactly where probability lies.\n",
    "\n",
    "## Why $\\sigma(a) ?$\n",
    "\n",
    "we want to model the log odds of $log(\\frac{p(c=c_1 | x=x_i)}{p(c=c_2 | x=x_i)}) = log(\\frac{p(c=c_1 | x=x_i)}{1 - p(c=c_1 | x=x_i)})\n",
    "= w^Tx_i = a$\n",
    "\n",
    "$\\implies \\frac{p(c=c_1 | x=x_i)}{1 - p(c=c_1 | x=x_i)} = e^a \\implies p(c=c_1 | x=x_i) = \\frac{e^a}{1 + e^a} = \\frac{1}{1 + e^{-a}}$\n",
    "\n",
    "## Next\n",
    "\n",
    "The output of the classifier can be $c_1, c_2$, we want to know the probability a sample is belonging to a class, that is\n",
    "we want to know $p(C=c_1 | X=x_i) \\text{ and }p(C=c_2 | X=x_i)$\n",
    "\n",
    "Let $p(C=c_1 | X=x_i) = \\sigma(w^Tx) = \\frac{1}{1 + e^{-w^Tx_i}}$, then $p(C=c_2 | X=x_i) = 1 - p(C=c_1 | X=x_i) = 1 - \\sigma(w^Tx_i)\n",
    "= 1 - \\frac{1}{1 + e^{-w^Tx_i}} = \\frac{e^{-w^Tx_i}}{1 + e^{-w^Tx_i}}$\n",
    "\n",
    "Where $w = [b, w^1, ..., w^d]^T$, $x = [1, x^1, ...., x^d]^T$\n",
    "\n",
    "Now, we suppose we have N samples, $\\{x_i, t_i\\}$, we want to use MLE to estimate the model parameter w.\n",
    "Let $y_i = 0$ for $t_i = C_1$ and $y_i = 1$ for $t_i = C_2$\n",
    "\n",
    "The likelihood $L(w)$:\n",
    "\n",
    "$L(w) = \\prod^{N}_{i=1} p(C=t_i | X=x_i; w)$\n",
    "\n",
    "$\\implies l(w) = \\sum_{i=1}^{N}log(p(C=t_i | X=x_i; w)) = y_i log(p(C=c_2 | X=x_i; w)) + (1 - y_i) log(p(C=c_1 | X=x_i; w))$\n",
    "\n",
    "Now, instead of maximize the log likelihood, we can minimize $-l(w)$:\n",
    "\n",
    "$E_{ce}(w) = -l(w) = - \\sum_{i=1}^{N}log(p(C=t_i | X=x_i; w)) = y_i log(p(C=c_2 | X=x_i; w)) + (1 - y_i) log(p(C=c_1 | X=x_i; w))$ (this $-l(w)$ is called **cross entropy**)\n",
    "\n",
    "\\begin{aligned}\n",
    "\\implies \\frac{\\partial E_{ce}(w)}{\\partial w} &= -\\frac{\\partial}{\\partial w} \\sum^{N}_{i=1} y_i log(\\frac{e^{w^Tx_i}}{1 + e^{-w^Tx_i}}) + (1 - y_i) log(\\frac{1}{1 + e^{-w^Tx_i}})\\\\\n",
    "&= -\\sum^{N}_{i=1} y_i(-x_i) - y_i log(1 + e^{-w^T x_i}) - (1 - y_i) log(1 + e^{-w^T x_i})\\\\\n",
    "&= -\\sum^{N}_{i=1} y_i(-x_i) + y_i x_i e^{-w^T x_i}\\frac{1}{1 + e^{-w^Tx_i}} + (1 - y_i) x_i e^{-w^T x_i}\\frac{1}{1 + e^{-w^Tx_i}}\\\\\n",
    "&= \\sum^{N}_{i=1} y_i(x_i) - x_i \\frac{e^{-w^T x_i}}{1 + e^{-w^Tx_i}}\\\\\n",
    "&= \\sum^{N}_{i=1} x_i( y_i - \\frac{e^{-w^T x_i}}{1 + e^{-w^Tx_i}})\\\\\n",
    "&= \\sum^{N}_{i=1} x_i( y_i - p(C=c_2 | X=x_i))\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Since this is a non-linear function of w, we do not have a direct solution for w, but we can use iterative method to update w\n",
    "\n",
    "## Multi-class\n",
    "\n",
    "Now, instead of two classes, we have K classes that we want to classify our sample $x_i$, that is, we want to know\n",
    "$p(C=c_1 | X=x_i)$, $p(C=c_2 | X=x_i)$, ... , $p(C=c_K | X=x_i)$:\n",
    "\n",
    "Define $p(C=c_i | X=x_n) = \\frac{e^{w_i^Tx_n}}{\\sum_{j=1}^{K} e^{w_j^T x_n}}$ (softmax function), we can clearly see that\n",
    "$\\sum_{i=1}^{K}p(C=c_i | X=x_n) = 1$\n",
    "\n",
    "We first one hot code our target class $c_j$, s.t class $c_j = [t_1, t_2, ...., t_K]^T, t_i\\in\\{0, 1\\}$ and only $t_j = 1$\n",
    "(i.e $c_3 = [0, 0, 1, 0, ...., 0]^T$)\n",
    "\n",
    "Then, we want to use MLE to find the corresponding parameters $w_1, w_2, w_3, ..., w_K$ for our probability distribution given N\n",
    "sample pairs $\\{x_i, g_i\\}$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$L(w_1, ..., w_K) = \\prod_{n=1}^{N} p(C=g_i | X=x_n; w_1, ..., w_K)$\n",
    "\n",
    "\\begin{aligned}\n",
    "\\implies -l(w_1, ..., w_K) & = -\\sum_{n=1}^{N} log(p(C=g_i | X=x_n; w_1, ..., w_K))\\\\\n",
    "&= -\\sum_{n=1}^{N} t_{n1}log(p(C=c_1 | X=x_n;w_1, ..., w_K)) +, ..., + t_{nK}log(p(C=c_K | X=x_n;w_1, ..., w_K))\\\\\n",
    "&= -\\sum^{N}_{n=1} \\sum^{K}_{i=1} t_{ni} log(p(C=c_i | X=x_n; w_1, ..., w_k))\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "The final equation is called multi-class cross entropy\n",
    "\n",
    "By taking the derivative w.r.p to the parameters $w_1, ..., w_K$ we have:\n",
    "\n",
    "$\\frac{\\partial -l(w_1, ..., w_K)}{\\partial w_i} = \\sum_{n=1}^{N} [p(C=c_i | X=x_n; w_1, ..., w_K) - t_{ni}]x_n$\n",
    "\n",
    "Also, for l2 regularization\n",
    "\n",
    "$\\frac{\\partial -l(w_1, ..., w_K)}{\\partial w_i} = \\sum_{n=1}^{N} [p(C=c_i | X=x_n; w_1, ..., w_K) - t_{ni}]x_n + \\lambda w_i$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from optimizer import SGD, Momentum\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, optimizer=SGD, c=1, max_iter=2000, lr=0.01, tor=0.0000001, verbose=False):\n",
    "\n",
    "        self.lr = lr\n",
    "        self.c = c\n",
    "        self.max_iter = max_iter\n",
    "        self.optimizer = optimizer\n",
    "        self.tor = tor\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.weights = None\n",
    "        self.k = None\n",
    "        self.d = None\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "\n",
    "        n_y, self.k = y_train.shape\n",
    "        x_train = np.column_stack([np.ones(n_y), x_train])\n",
    "        n_x, self.d = x_train.shape\n",
    "        i = 0\n",
    "\n",
    "        if not self.weights:\n",
    "            self.weights = np.random.randn(self.d, self.k)\n",
    "\n",
    "        opt = self.optimizer(lr=self.lr, model_vars=[self.weights])\n",
    "\n",
    "        prev_matrix = 0\n",
    "        dif = np.linalg.norm(self.weights - prev_matrix)\n",
    "\n",
    "        if self.k > 1:\n",
    "\n",
    "            while (i <= self.max_iter) and (dif >= self.tor):\n",
    "\n",
    "                prev_matrix = self.weights\n",
    "\n",
    "                if self.verbose and (i % self.verbose == 0):\n",
    "                    print(f'iteration: {i}, loss: {self._cal_train_loss(x_train, y_train)}')\n",
    "\n",
    "                self.weights = opt([self._mc_ce_grad(x_train, y_train)])[0]\n",
    "                dif = np.linalg.norm(self.weights - prev_matrix)\n",
    "                i += 1\n",
    "\n",
    "    def predict(self, x_test):\n",
    "\n",
    "        x_test = np.column_stack([np.ones(x_test.shape[0]), x_test])\n",
    "        pred = [np.argmax(self._cal_softmax(x_i)) for x_i in x_test]\n",
    "            \n",
    "        return pred\n",
    "\n",
    "\n",
    "    def _mc_ce_grad(self, x, labels):\n",
    "\n",
    "        n = x.shape[0]\n",
    "        output_g = np.zeros((self.d, self.k))\n",
    "\n",
    "        for i, x_i in enumerate(x):\n",
    "\n",
    "            soft_max = self._cal_softmax(x_i)\n",
    "            temp_g = np.repeat(x_i.reshape(-1, 1), self.k, axis=1) * (soft_max - labels[i])\n",
    "            output_g += temp_g\n",
    "\n",
    "        return (output_g + self.c * self.weights)/ n\n",
    "\n",
    "    def _cal_softmax(self, x):\n",
    "\n",
    "        bot = 0\n",
    "        top = []\n",
    "\n",
    "        for k in range(self.k):\n",
    "\n",
    "            ea = np.exp(np.dot(self.weights[:, k], x))\n",
    "            bot += ea\n",
    "            top.append(ea)\n",
    "\n",
    "        return np.array(top / bot)\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy(y_pred, y_true):\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for i, v in enumerate(y_true):\n",
    "\n",
    "            y_pred_i = np.log(y_pred[i])\n",
    "            loss += np.dot(y_pred_i, v.T)\n",
    "\n",
    "        return -loss / len(y_true)\n",
    "\n",
    "    def _cal_train_loss(self, x_train, y_train):\n",
    "        \n",
    "        y_pred = self._cal_softmax(x_train[0])\n",
    "\n",
    "        for i in x_train[1:]:\n",
    "            \n",
    "            y_pred = np.row_stack([y_pred, self._cal_softmax(i)])\n",
    "            \n",
    "        return self.cross_entropy(y_pred, y_train)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "x = load_iris()['data']\n",
    "y = load_iris()['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "y_train = OneHotEncoder().fit_transform(y.reshape(-1,1)).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000, optimizer=Momentum, verbose=100)\n",
    "lr_sgd = LogisticRegression(max_iter=1000, optimizer=SGD, verbose=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 5.098582316688093\n",
      "iteration: 100, loss: 0.3517027932892621\n",
      "iteration: 200, loss: 0.28276999963179617\n",
      "iteration: 300, loss: 0.2456013513841422\n",
      "iteration: 400, loss: 0.22242954025713682\n",
      "iteration: 500, loss: 0.20679155779461852\n",
      "iteration: 600, loss: 0.1956724764944553\n",
      "iteration: 700, loss: 0.18747024379886304\n",
      "iteration: 800, loss: 0.181253620758808\n",
      "iteration: 900, loss: 0.17644388709793463\n",
      "iteration: 1000, loss: 0.17266229070230382\n"
     ]
    }
   ],
   "source": [
    "lr.fit(x, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 11.220214124414223\n",
      "iteration: 100, loss: 1.2584502937847382\n",
      "iteration: 200, loss: 0.7440648809633511\n",
      "iteration: 300, loss: 0.5916334654839972\n",
      "iteration: 400, loss: 0.5207964125813188\n",
      "iteration: 500, loss: 0.47726521907148084\n",
      "iteration: 600, loss: 0.4461981568121418\n",
      "iteration: 700, loss: 0.422029801982726\n",
      "iteration: 800, loss: 0.402202417434294\n",
      "iteration: 900, loss: 0.38536475871017123\n",
      "iteration: 1000, loss: 0.3707261901775692\n"
     ]
    }
   ],
   "source": [
    "lr_sgd.fit(x, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "pred = lr.predict(x)\n",
    "pred_sgd = lr_sgd.predict(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\msi\\.conda\\envs\\unsupervised_learning\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": "LogisticRegression()"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_lr = LR()\n",
    "sklearn_lr.fit(x, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "pred_sklearn = sklearn_lr.predict(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "0.98"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9733333333333334"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred_sgd, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9733333333333334"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred_sklearn, y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}