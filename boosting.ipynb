{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Boosting\n",
    "\n",
    "## Forward stagewise additive modeling\n",
    "Generally, forward stagewise additive modeling takes the form:\n",
    "\n",
    "$f(x) = \\sum_{m=1}^{M} \\beta_m b_m(x; \\gamma_m)$\n",
    "\n",
    "Where $\\beta_i$ are the expansion coefficients and $b_i(x; \\gamma_i)$ are usually simple functions of $x$ (i.e a weak classifier)\n",
    "### Algorithm\n",
    "\n",
    "1. initialize $f_o(x) = 0$\n",
    "2. for m = 1 to M:\n",
    " - a. Compute: $(\\beta_m, \\gamma_m) = \\text{argmin}_{\\beta, \\gamma} \\sum^{N}_{i=1} L(y_i, f_{m-1} (x_i) + \\beta b(x_i; \\gamma))$\n",
    " - b. set $f_m = f_{m-1} (x) + \\beta_m b(x; \\gamma_m)$\n",
    "\n",
    "For $b_i(x; \\gamma_i)$ = tree, $\\gamma_i$ can be splitting variables and splitting points\n",
    "\n",
    "Forward stagewise modeling approximates the solution to 2.a by  sequentially adding new basis function to the expansion without\n",
    "adjusting the parameters and coefficients of those that have already been added. At each iteration m, one solves for the\n",
    "optimal basis function $b_m(x; \\gamma_m)$ and coefficient $\\beta_m$, adds it to the current expansion $f_{m-1} (x)$, previously added\n",
    "terms are not modified.\n",
    "\n",
    "## Adaboost as forward additive stagewise modeling\n",
    "\n",
    "Assume, we have a two class classification problem with class labels $y \\in \\{-1, 1\\}$ and we want to build up from some weak classifiers $\\{b_i\\}$\n",
    "that only do slightly better than random guess.\n",
    "\n",
    "Let the loss function $L = e^{-yf(x)}$\n",
    "\n",
    "For forward additive stagewise modeling, at each iteration m, one must solve\n",
    "\n",
    "$(\\beta_m, b_m) = \\text{argmin}_{\\beta, b} \\sum_{i=1}^{N} e^{-y[f_{m-1}(x_i) + \\beta b(x_i)]}$\n",
    "\n",
    "\\begin{aligned}\n",
    "\\implies (\\beta_m, b_m) & = \\text{argmin}_{\\beta, b} \\sum_{i=1}^{N} \\underbrace{e^{-y[f_{m-1}(x_i)]}}_{\\text{this term does not depend on $\\beta, b$}}e^{-y[\\beta b(x_i)]}\\\\\n",
    "& = \\text{argmin}_{\\beta, b} \\sum_{i=1}^{N} w_i^{(m)}e^{-y[\\beta b(x_i)]}\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Where $w_i^{(m)} = e^{-y[f_{m-1}(x_i)]}$, we can regard this as a weight applied on each sample at each iteration\n",
    "\n",
    "For any value of $\\beta$,\n",
    "\n",
    "  \\begin{equation}\n",
    "    w_i^{(m)}e^{-y[\\beta b(x_i)]} =\n",
    "    \\begin{cases}\n",
    "      w_i^{(m)} e^{-\\beta} & y_i = b(x_i)\\\\\n",
    "      w_i^{(m)} e^{\\beta}, & y_i \\neq b(x_i)\n",
    "    \\end{cases}\n",
    "  \\end{equation}\n",
    "\n",
    "\\begin{aligned}\n",
    "\\implies \\sum_{i=1}^{N} w_i^{(m)}e^{-y[\\beta b(x_i)]} & = e^\\beta \\sum_{i=1}^{N} w_i^{(m)}I(y_i \\neq b(x_i)) + e^{-\\beta}\\sum_{i=1}^{N} w_i^{(m)}I(y_i = b(x_i))\\\\\n",
    "& =  e^\\beta \\sum_{i=1}^{N} w_i^{(m)}I(y_i \\neq b(x_i)) + e^{-\\beta}(\\sum_{i=1}^{N} w_i^{(m)} - \\sum_{i=1}^{N}w_i^{(m)}I(y_i \\neq b(x_i)))\\\\\n",
    "& = (e^\\beta - e^{-\\beta}) \\sum_{i=1}^{N} w_i^{(m)}I(y_i \\neq b(x_i)) + e^{-\\beta} \\sum_{i=1}^{N} w_i^{(m)}\n",
    "\\end{aligned}\n",
    "\n",
    "$\\implies b_m(x_i) = \\text{argmin}_{b} \\sum_{i=1}^{N} w_i^{(m)}I(y_i \\neq b(x_i))$ which is a classifier $b_m$ that minimizes weighted classification errors\n",
    "\n",
    "By taking the derivative wrt $\\beta$:\n",
    "\n",
    "$\\implies(e^\\beta - e^{-\\beta}) \\sum_{i=1}^{N} w_i^{(m)}I(y_i \\neq b(x_i)) + e^{-\\beta} \\sum_{i=1}^{N} w_i^{(m)} = 0$\n",
    "\n",
    "$\\implies \\frac{e^\\beta + e^{-\\beta}}{e^{-\\beta}} = \\frac{\\sum_{i=1}^{N} w_i^{(m)}}{\\sum_{i=1}^{N} w_i^{(m)}I(y_i \\neq b(x_i))}$\n",
    "\n",
    "$\\implies \\frac{e^\\beta}{e^{-\\beta}} = \\frac{\\sum_{i=1}^{N} w_i^{(m)}}{\\sum_{i=1}^{N} w_i^{(m)}I(y_i \\neq b(x_i))} - 1$\n",
    "\n",
    "$\\implies 2\\beta = log(\\frac{1}{error_m} - 1)$\n",
    "\n",
    "$\\implies \\beta_m = \\frac{1}{2} log (\\frac{1 - error_m}{error_m})$\n",
    "\n",
    "Where, $error_m = \\frac{\\sum_{i=1}^{N} w_i^{(m)}I(y_i \\neq b(x_i))}{\\sum_{i=1}^{N} w_i^{(m)}}$\n",
    "\n",
    "Since, $f_m(x) = f_{m-1}(x) + \\beta_m b_m(x) \\implies w_i^{(m + 1)} = e^{-y[f_{m}(x_i)]} = e^{-yf_{m-1}(x) + \\beta_m b_m(x)} = w_i^{(m)} e^{-y_i\\beta_mb_m(x_i)}$\n",
    "\n",
    "This means that the weight applied on each sample is adjusted by a factor of $e^{-y_i\\beta_mb_m(x_i)}$\n",
    "\n",
    "## Adaboost\n",
    "\n",
    "Even though Adaboost algorithm was originally motivated from a very different perspective than presented in the previous section, it is equivalent to\n",
    "forward stagewise additive modeling based on exponential loss.\n",
    "\n",
    "### Algorithm\n",
    "1. Initialize the observation weights $w_i^{(1)} = \\frac{1}{N}, i\\in \\{1, ...., N\\}$\n",
    "2. For m = 1 to M:\n",
    " - a. Fit a classifier $b_m(x)$ to the training data using weights $w_i$\n",
    " - b. Compute $error_m = \\frac{\\sum_{i=1}^{N} w_i^{(m)}I(y_i \\neq b(x_i))}{\\sum_{i=1}^{N} w_i^{(m)}}$\n",
    " - c. Compute $\\beta_m = \\frac{1}{2} log (\\frac{1 - error_m}{error_m})$\n",
    " - d. Set $w_i^{(m + 1)} = w_i^{(m)} e^{-y_i\\beta_mb_m(x_i)}, \\forall i \\in \\{1, ..., N\\}$\n",
    "\n",
    "3. Output $f(x) = sign[\\sum_{i=1}^{M} \\beta_m b_m(x)]$\n",
    "\n",
    "At each iteration, weight of each sample is multiplied by:\n",
    "1. $e^\\beta$ if misclassified\n",
    "2. $e^{-\\beta}$ if classified correctly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}